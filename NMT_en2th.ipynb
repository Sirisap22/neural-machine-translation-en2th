{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_en2th.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Hb8-0u2sNA"
      },
      "source": [
        "# import Pytorch\r\n",
        "TODO\r\n",
        "- train with smaller datasets\r\n",
        "- train with smaller hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiOduN3S2u5x"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwLdKiC-PBrY"
      },
      "source": [
        "# tokenizer\r\n",
        "def installPyThaiNLP():\r\n",
        "    !pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\r\n",
        "    !pip install epitran\r\n",
        "    !pip install sklearn_crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0R-BZs8PMhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69e49e3-336a-43bd-ffb3-b3a959d65e18"
      },
      "source": [
        " installPyThaiNLP() # run once"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
            "  Using cached https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
            "Requirement already satisfied (use --upgrade to upgrade): pythainlp==2.3.0.dev0 from https://github.com/PyThaiNLP/pythainlp/archive/dev.zip in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from pythainlp==2.3.0.dev0) (0.9.7)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from pythainlp==2.3.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tinydb>=3.0 in /usr/local/lib/python3.6/dist-packages (from pythainlp==2.3.0.dev0) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp==2.3.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp==2.3.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp==2.3.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp==2.3.0.dev0) (1.24.3)\n",
            "Building wheels for collected packages: pythainlp\n",
            "  Building wheel for pythainlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pythainlp: filename=pythainlp-2.3.0.dev0-cp36-none-any.whl size=11003368 sha256=0dc9c4f347f96ffca2a411357fce20b45c9adb8f13ad664cf03cfcfdf721df86\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x_erra0d/wheels/79/4e/1e/26f3198c6712ecfbee92928ed1dde923a078da3d222401cc78\n",
            "Successfully built pythainlp\n",
            "Requirement already satisfied: epitran in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from epitran) (2019.12.20)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.6/dist-packages (from epitran) (0.7.5)\n",
            "Requirement already satisfied: panphon>=0.16 in /usr/local/lib/python3.6/dist-packages (from epitran) (0.17)\n",
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python3.6/dist-packages (from epitran) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from epitran) (53.0.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (0.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (3.13)\n",
            "Requirement already satisfied: munkres in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (1.19.5)\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdkgEO_-604y"
      },
      "source": [
        "# Machine translation\r\n",
        "## Handling Text\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Vj3PTik3iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bc4e9f-063f-4078-c718-a607793481cb"
      },
      "source": [
        "def getEncoding(path):\r\n",
        "    with open(path) as f:\r\n",
        "        print(f.encoding)\r\n",
        "getEncoding(\"nmt_data/generated_reviews_translator.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UTF-8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrCMfkoOMxC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606b2b04-78b1-481e-bd51-2cf54e7fa0e2"
      },
      "source": [
        "def readAndSeperateSourceAndTarget(path, encoding):\r\n",
        "    data = pd.read_csv(path, encoding=encoding)\r\n",
        "    return data[\"en_text\"], data[\"th_text\"]\r\n",
        "\r\n",
        "source_sentence_raw, target_sentence_raw = readAndSeperateSourceAndTarget(\"nmt_data/generated_reviews_translator.csv\", getEncoding(\"nmt_data/generated_reviews_translator.csv\"))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UTF-8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOfl2OUpNWYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b23ca6-71bc-49a0-addf-af85340836f6"
      },
      "source": [
        "source_sentence_raw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    These are a terrible product from China.The li...\n",
              "1    They can't be used for anything other than par...\n",
              "2    And yes, I followed directions exactly as they...\n",
              "3    What kind of trash can cannot survive simple h...\n",
              "4                                  I'm returning them.\n",
              "Name: en_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02KI2HFGOKAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f02eda-cbf1-4fa7-dc83-a2a6ff630dc0"
      },
      "source": [
        "target_sentence_raw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    สินค้าพวกนี้เป็นสินค้าห่วยๆที่มาจากประเทศจีน ท...\n",
              "1     ไม่สามารถใช้ทำอะไรได้มากกว่าใช้ประดับในงานเลี้ยง\n",
              "2    และแน่นอนว่าฉันได้ทำตามแนวทางที่เขาเขียนไว้อย่...\n",
              "3    กระป๋องขยะประเภทไหนกันที่ไม่สามารถใช้งานง่ายๆใ...\n",
              "4                          ฉันจะส่งสินค้าพวกนี้กลับค่ะ\n",
              "Name: th_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5DkjuT08Pwx"
      },
      "source": [
        "Maps word into ID numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06UC-52r7NtR"
      },
      "source": [
        "class Vocabulary:\r\n",
        "    def __init__(self):\r\n",
        "        self.word2id = { \"<s>\": 0, \"</s>\": 1 }\r\n",
        "        self.id2word = { 0: \"<s>\", 1: \"</s>\" }\r\n",
        "        self.n_words = 2\r\n",
        "        # with open(file_name) as f:\r\n",
        "        #     for line in f:\r\n",
        "        #         (word, id) = line.split()\r\n",
        "        #         if int(id) == 1 or int(id) == 2:\r\n",
        "        #             continue\r\n",
        "        #         self.word2id[word] = int(id)\r\n",
        "        #         self.id2word[int(id)] = word\r\n",
        "        #         self.n_words += 1\r\n",
        "    def getID(self, word):\r\n",
        "        if word not in self.word2id:\r\n",
        "            self.word2id[word] = self.n_words\r\n",
        "            self.id2word[self.n_words] = word\r\n",
        "            self.n_words += 1\r\n",
        "        return self.word2id[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HGtULDzVCNv"
      },
      "source": [
        "Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOk5XZHUUjoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2703c5-6883-4d24-f83f-5d9ce6762b6d"
      },
      "source": [
        "def runOnceNltk():\r\n",
        "    import nltk\r\n",
        "    nltk.download('punkt')\r\n",
        "runOnceNltk()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Crn3O2MRmg"
      },
      "source": [
        "from pythainlp import word_tokenize as tokenize_thai\r\n",
        "from nltk.tokenize import word_tokenize as tokenize_eng\r\n",
        "import re\r\n",
        "\r\n",
        "def tokenizeThai(s):\r\n",
        "    return tokenize_thai(s, keep_whitespace=False)\r\n",
        "def tokenizeEng(s):\r\n",
        "    return tokenize_eng(s)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je4zUA5VSR-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945a5679-e276-48df-cc9d-8f01b3b82a74"
      },
      "source": [
        "print(tokenizeThai(target_sentence_raw[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['สินค้า', 'พวก', 'นี้', 'เป็น', 'สินค้า', 'ห่วย', 'ๆ', 'ที่', 'มาจาก', 'ประเทศ', 'จีน', 'ทั้ง', 'หลอดไฟ', 'และ', 'สกรู', 'เหล็ก', 'ที่', 'ที่', 'ใส่', 'เข้าไป', 'ใน', 'แก้ว', 'นั้น', 'ไม่', 'มี', 'แม้แต่', 'ความพอดี', '…', 'มัน', 'จึง', 'เกือบ', 'ที่จะ', 'เป็นไปไม่ได้', 'เลย', 'ที่', 'กระป๋อง', 'พวก', 'นี้', 'จะ', 'ไม่', 'แตก', 'ออก', 'เป็น', 'ชิ้น', 'เล็ก', 'ๆจน', 'ส่ง', 'ผลเสีย', 'ต่อ', 'อาหาร', 'ของ', 'คุณ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zihWeBeTCDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af272e7b-c0bd-44c8-ddb0-411bccc1aac4"
      },
      "source": [
        "print(tokenize_eng(source_sentence_raw[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['These', 'are', 'a', 'terrible', 'product', 'from', 'China.The', 'light', 'bulb', 'and', 'the', 'metal', 'screw', 'that', 'goes', 'into', 'the', 'glass', 'does', \"n't\", 'even', 'fit', 'properly', '...', 'so', 'it', 'is', 'almost', 'impossible', 'to', 'open', 'these', 'cans', 'without', 'breaking', 'a', 'very', 'small', 'part', 'off', 'or', 'risking', 'ruining', 'your', 'food', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaXbi3jU8mn"
      },
      "source": [
        "def convertWord2id(data, tokenizer):\r\n",
        "    vocab = Vocabulary()\r\n",
        "    idData = []\r\n",
        "    for index, sentence in data.items():\r\n",
        "        sentence = addSOSandEOS(tokenizer(sentence))\r\n",
        "        temp = []\r\n",
        "        for word in sentence:\r\n",
        "            temp.append(vocab.getID(word))\r\n",
        "        idData.append(temp)\r\n",
        "    return idData, vocab\r\n",
        "def addSOSandEOS(sentence):\r\n",
        "    sentence.insert(0, \"<s>\")\r\n",
        "    sentence.append(\"</s>\")\r\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNHfTGcfXJR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87a5925-9125-4d3c-dbaf-f2ed835df0c2"
      },
      "source": [
        "prepared_target_sentence, target_vocab = convertWord2id(target_sentence_raw, tokenize_thai)\r\n",
        "print(prepared_target_sentence[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 8, 8, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 8, 34, 3, 4, 35, 23, 36, 37, 5, 38, 39, 40, 41, 42, 43, 44, 45, 46, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGFwlKenX1pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0650afe8-d9b4-42c1-c072-1cbf54c76f33"
      },
      "source": [
        "prepared_source_sentence, source_vocab = convertWord2id(source_sentence_raw, tokenize_eng)\r\n",
        "print(prepared_source_sentence[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 12, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 4, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7qjU3p2WwB"
      },
      "source": [
        "\r\n",
        "## Encoder-Decoder Approach (RNN with Attention)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2g0Pl8Le2v"
      },
      "source": [
        "class Encoder(torch.nn.Module):\r\n",
        "    def __init__(self, vocab_size, hidden_size, max_length):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.max_length = max_length\r\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\r\n",
        "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, input, hidden):\r\n",
        "        embedded = self.embedding(torch.tensor([[input]])).view(1, 1, -1)\r\n",
        "        output, hidden = self.gru(embedded, hidden)\r\n",
        "        return output, hidden\r\n",
        "    def process_sentence(self, input_sentence):\r\n",
        "        \"\"\"Processing of an entire input sentence\"\"\"\r\n",
        "        hidden = torch.zeros(1, 1, self.hidden_size)\r\n",
        "        encoder_outputs = torch.zeros(self.max_length, self.hidden_size)\r\n",
        "        for i in range(len(input_sentence)):\r\n",
        "            embedded = self.embedding(torch.tensor([[input_sentence[i]]])).view(1, 1, -1)\r\n",
        "            output, hidden = self.gru(embedded, hidden)\r\n",
        "            encoder_outputs[i] = output[0, 0]\r\n",
        "        return encoder_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZCo3weh40lX"
      },
      "source": [
        "class Decoder(torch.nn.Module):\r\n",
        "    def __init__(self, vocab_size, hidden_size, max_length):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.max_length = max_length\r\n",
        "        self.gru = torch.nn.GRU(2 * hidden_size, hidden_size)\r\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\r\n",
        "        self.Wa = torch.nn.Linear(hidden_size, hidden_size, bias=False)\r\n",
        "        self.Ua = torch.nn.Linear(hidden_size, hidden_size, bias=False)\r\n",
        "        self.va = torch.nn.Parameter(torch.FloatTensor(1,hidden_size))\r\n",
        "        self.out = torch.nn.Linear(3 * hidden_size, vocab_size)\r\n",
        "\r\n",
        "    def forward(self, prev_output_id, prev_hidden, encoder_output,input_length):\r\n",
        "        prev_output = self.embedding(torch.tensor([prev_output_id])).unsqueeze(1)\r\n",
        "        \r\n",
        "        m = torch.tanh(self.Wa(prev_hidden) + self.Ua(encoder_output))\r\n",
        "        attention_scores = m.bmm(self.va.unsqueeze(2)).squeeze(-1)\r\n",
        "        attention_scores = self.mask(attention_scores, input_length)\r\n",
        "        attention_weights = torch.nn.functional.softmax( attention_scores, -1 )\r\n",
        "\r\n",
        "        context = attention_weights.unsqueeze(1).bmm(encoder_output.unsqueeze(0))\r\n",
        "\r\n",
        "        rnn_input = torch.cat((prev_output, context), 2)\r\n",
        "        rnn_output, hidden = self.gru(rnn_input, prev_hidden)\r\n",
        "\r\n",
        "        output = self.out(torch.cat((rnn_output, context, prev_output), 2))\r\n",
        "        output = torch.nn.functional.log_softmax(output[0], dim=1)\r\n",
        "        return output, hidden\r\n",
        "\r\n",
        "    def mask(self, scores, input_length):\r\n",
        "        s = scores.squeeze(0)\r\n",
        "        for i in range(self.max_length-input_length):\r\n",
        "            s[input_length+i] = -float('inf')\r\n",
        "        return s.unsqueeze(0)\r\n",
        "    def initHidden(self):\r\n",
        "        return torch.zeros(1, 1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJBEREDqKPOn"
      },
      "source": [
        "def train(source_vocab, target_vocab, source_corpus, target_corpus):\r\n",
        "    criterion = torch.nn.NLLLoss()\r\n",
        "    hidden_size = 256\r\n",
        "    learning_rate = 0.01\r\n",
        "\r\n",
        "    # for visualize\r\n",
        "    loss = []\r\n",
        "    \r\n",
        "    # initialize model\r\n",
        "    encoder = Encoder(source_vocab.n_words, hidden_size, 100)\r\n",
        "    decoder = Decoder(target_vocab.n_words, hidden_size, 100)\r\n",
        "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\r\n",
        "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "    for epoch in range(10):\r\n",
        "        total_loss_epoch = 0\r\n",
        "        for source_sentence, target_sentence in zip(source_corpus, target_corpus):\r\n",
        "            encoder_optimizer.zero_grad()\r\n",
        "            decoder_optimizer.zero_grad()\r\n",
        "\r\n",
        "            encoder_output = encoder.process_sentence(source_sentence)\r\n",
        "\r\n",
        "            sentence_loss = 0\r\n",
        "            hidden = decoder.initHidden()\r\n",
        "            for i in range(len(target_sentence)-1):\r\n",
        "                output, hidden = decoder.forward(target_sentence[i], hidden, encoder_output, len(source_sentence))\r\n",
        "                word_loss = criterion(output, torch.tensor([target_sentence[i+1]]))\r\n",
        "                sentence_loss += word_loss\r\n",
        "\r\n",
        "            total_loss_epoch += sentence_loss\r\n",
        "            \r\n",
        "            sentence_loss.backward()\r\n",
        "            encoder_optimizer.step()\r\n",
        "            decoder_optimizer.step()\r\n",
        "        # loss.append(total_loss_epoch/len(source_corpus))\r\n",
        "\r\n",
        "    return encoder, decoder, encoder_optimizer, decoder_optimizer, loss\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tx5Vl_fY7L6"
      },
      "source": [
        "# ram ไม่พอ gg\r\n",
        "trained_encoder, trained_decoder, _en_op, _de_op, loss = train(source_vocab, target_vocab, prepared_source_sentence, prepared_target_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVH9azvKNVbG"
      },
      "source": [
        "torch.save(trained_encoder.state_dict(), \"models/encoder.pt\")\r\n",
        "torch.save(trained_decoder.state_dict(), \"models/decoder.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}